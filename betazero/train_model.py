import tensorflow as tf
import sys
import pickle
import tf_keras as keras
import os
import matplotlib.pyplot as plt

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

def load_model(dir_name):
    model = keras.models.load_model(dir_name)
    optimizer = keras.optimizers.Adam(learning_rate=1e-5)
    accuracy_metric = keras.metrics.CategoricalAccuracy()
    model.compile(optimizer=optimizer, loss={"policy_output": "categorical_crossentropy", "value_output": "categorical_crossentropy"}, metrics={"policy_output": ["mae", "accuracy"], "value_output": ["mae", accuracy_metric]})
    return model


def prepare_games(moves, inputs, p_outputs, v_outputs):
    for move in moves:
        input = tf.constant(move["board"]["data"], name="input_1")
        input = tf.cast(tf.reshape(input, (8, 8, 14)), tf.float32)
        inputs.append(input)
        p_outputs.append(tf.constant(move["moves"]))
        v_outputs.append(tf.constant(move["won"]))


def load_and_train_dir(model, dir_name="./games"):
    inputs, p_outputs, v_outputs = [], [], []
    for (dirpath, _, file_names) in os.walk(dir_name):
        for name in file_names:
            print("Adding file " + os.path.join(dirpath, name))
            with open(os.path.join(dirpath, name), "rb") as f:
                games = pickle.load(f)
                prepare_games(games, inputs, p_outputs, v_outputs)
            dataset = tf.data.Dataset.from_tensor_slices((inputs, {"policy_output": p_outputs, "value_output": v_outputs })).shuffle(5000).batch(8)
            yield model.fit(dataset, epochs=10, shuffle=True)


def load_and_train(model, file_name="latest.pickle"):
    inputs, p_outputs, v_outputs = [], [], []
    with open(file_name, "rb") as f:
        games = pickle.load(f)
        prepare_games(games, inputs, p_outputs, v_outputs)
    dataset = tf.data.Dataset.from_tensor_slices((inputs, {"policy_output": p_outputs, "value_output": v_outputs })).shuffle(5000).batch(8)

    return model.fit(dataset, epochs=10, shuffle=True)


def save_history(history, losses, p_loss, v_loss):
    for l in history.history["loss"]:
        losses.append(l)
    for p in history.history["policy_output_loss"]:
        p_loss.append(p)
    for v in history.history["value_output_loss"]:
        v_loss.append(v)
    with open ("losses.txt", "w") as f:
        f.writelines([str(losses) + "\n", str(p_loss) + "\n", str(v_loss)])


def plot_losses(losses_1, losses_2, title="Neural Network Training Loss"):
    print(len(losses_1))
    epochs = range(0, len(losses_1))
    ticks = range(0, len(losses_1) + 1, 5)
    plt.rc('font', size=30)
    
    fig, ax = plt.subplots()

    # Stacked bar chart
    ax.bar(epochs, losses_1, label="Policy Output Loss")
    ax.bar(epochs, losses_2, bottom = losses_1, label="Value Output Loss")
    ax.legend()
    
    # ax.set_title(title)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Loss")
    ax.grid(True)
    ax.set_xticks(ticks)
    # ax.tight_layout()  # Adjust layout to prevent labels from overlapping


    # plt.figure(figsize=(10, 6))
    # plt.plot(epochs, losses, marker='o', linestyle='-', color='blue')
    # plt.title(title)
    # plt.xlabel("Epoch")
    # plt.ylabel("Loss")
    # plt.grid(True)
    # plt.xticks(ticks)
    # plt.tight_layout()  # Adjust layout to prevent labels from overlapping
    plt.show()

# plot_losses([0.6298749446868896, 0.35933181643486023, 0.2122836709022522, 0.14791154861450195, 0.1158791035413742, 0.08341418206691742, 0.08034800738096237, 0.0743352398276329, 0.08219759911298752, 0.06408735364675522, 0.4008795917034149, 0.1402801126241684, 0.09983060508966446, 0.08850543946027756, 0.08030638843774796, 0.07123247534036636, 0.07426750659942627, 0.07616438716650009, 0.06113584339618683, 0.06786932051181793, 0.213307186961174, 0.10768169909715652, 0.09733434021472931, 0.09448733180761337, 0.07103853672742844, 0.07284373790025711, 0.07122794538736343, 0.06733998656272888, 0.07649789750576019, 0.06888697296380997, 0.127619206905365, 0.09037967026233673, 0.0744611993432045, 0.06546086072921753, 0.07137896865606308, 0.0655832290649414, 0.05661319941282272, 0.058235052973032, 0.06076745688915253, 0.05685899779200554, 0.13322798907756805, 0.06748417764902115, 0.05339416489005089, 0.0560685396194458, 0.0540311336517334, 0.051341377198696136, 0.05289394035935402, 0.05241155996918678, 0.04999333620071411, 0.04581739753484726, 0.12087894976139069, 0.06517516821622849, 0.055154986679553986, 0.06042345613241196, 0.05602867528796196, 0.055240776389837265, 0.05139589309692383, 0.049013182520866394, 0.05041072890162468, 0.0506930835545063, 0.1644442081451416, 0.06738152354955673, 0.058581747114658356, 0.05671878904104233, 0.05430356413125992, 0.05386405810713768, 0.052825186401605606, 0.04825897514820099, 0.05267955735325813, 0.04946718364953995, 0.10599679499864578, 0.06473784148693085, 0.057416774332523346, 0.05658017098903656, 0.052235644310712814, 0.057516247034072876, 0.05500503256917, 0.04995560646057129, 0.050416842103004456, 0.057274315506219864, 0.10098274052143097, 0.0666571781039238, 0.060495659708976746, 0.06087255850434303, 0.05260844528675079, 0.05422867089509964, 0.05721467360854149, 0.052244581282138824, 0.06446318328380585, 0.05509813129901886, 0.1098676398396492, 0.06633029878139496, 0.055566854774951935, 0.05758281424641609, 0.05477098748087883, 0.05030177906155586, 0.05684483423829079, 0.060923561453819275, 0.0536634624004364, 0.05471589416265488],
#             title="Betazero Model Training Loss")

plot_losses([0.05160736292600632, 0.03272311016917229, 0.02769358642399311, 0.024787554517388344, 0.022682955488562584, 0.02102239988744259, 0.0198766328394413, 0.018964486196637154, 0.018045229837298393, 0.017297247424721718, 0.023422690108418465, 0.019333485513925552, 0.01774190366268158, 0.01660742051899433, 0.015825888141989708, 0.015155631117522717, 0.014519239775836468, 0.014190290123224258, 0.013841361738741398, 0.0134264025837183, 0.015044954605400562, 0.013963566161692142, 0.013332775793969631, 0.012895778752863407, 0.012483829632401466, 0.012178517878055573, 0.012006809003651142, 0.011821596883237362, 0.01161478366702795, 0.011409145779907703, 0.012077255174517632, 0.011683434247970581, 0.011224685236811638, 0.010966761037707329, 0.010850979946553707, 0.010604914277791977, 0.010467772372066975, 0.010321610607206821, 0.010209066793322563, 0.010130595415830612, 0.010829562321305275, 0.01034526713192463, 0.009950212202966213, 0.00982604455202818, 0.009640593081712723, 0.009561347775161266, 0.009527773596346378, 0.009386218152940273, 0.009313936345279217, 0.009221791289746761, 0.01006512064486742, 0.009661701507866383, 0.009394754655659199, 0.009368143044412136, 0.009275675751268864, 0.009189469739794731, 0.009088297374546528, 0.008990658447146416, 0.008991354145109653, 0.009014945477247238, 0.01018067542463541, 0.00959062296897173, 0.009411513805389404, 0.009367712773382664, 0.009259629063308239, 0.009119219146668911, 0.009052734822034836, 0.00897225271910429, 0.008915032260119915, 0.008951193653047085, 0.011297081597149372, 0.010565673001110554, 0.010260113514959812, 0.010036451742053032, 0.009912820532917976, 0.009887863881886005, 0.009943407960236073, 0.00963687151670456, 0.009597480297088623, 0.009567514061927795, 0.011170906014740467, 0.01061851717531681, 0.010316181927919388, 0.010313425213098526, 0.010010739788413048, 0.009996584616601467, 0.010056160390377045, 0.009829373098909855, 0.010306759737432003, 0.010024146176874638, 0.010963624343276024, 0.0104447016492486, 0.010237527079880238, 0.010174611583352089, 0.010082653723657131, 0.0099480664357543, 0.009942849166691303, 0.010172445327043533, 0.010042646899819374, 0.009860258549451828], [0.5782675743103027, 0.3266085982322693, 0.18459010124206543, 0.12312400341033936, 0.09319615364074707, 0.06239180639386177, 0.06047137826681137, 0.0553707592189312, 0.06415237486362457, 0.04679011553525925, 0.3774568736553192, 0.12094663828611374, 0.0820886492729187, 0.07189799845218658, 0.0644804984331131, 0.05607682466506958, 0.059748269617557526, 0.06197406351566315, 0.047294486314058304, 0.054442938417196274, 0.1982623040676117, 0.09371814876794815, 0.08400154113769531, 0.08159155398607254, 0.05855468288064003, 0.060665227472782135, 0.05922114476561546, 0.05551839619874954, 0.06488312780857086, 0.05747784674167633, 0.11554189026355743, 0.07869622111320496, 0.0632365420460701, 0.05449412018060684, 0.06052800640463829, 0.05497833713889122, 0.04614540934562683, 0.047913458198308945, 0.05055839195847511, 0.046728380024433136, 0.12239842116832733, 0.057138897478580475, 0.04344399273395538, 0.046242501586675644, 0.04439052194356918, 0.04178005829453468, 0.04336615651845932, 0.04302532598376274, 0.04067939519882202, 0.03659560903906822, 0.11081380397081375, 0.055513471364974976, 0.04576023295521736, 0.051055338233709335, 0.04675300419330597, 0.04605131596326828, 0.04230761528015137, 0.04002254828810692, 0.04141939803957939, 0.041678134351968765, 0.15426352620124817, 0.057790905237197876, 0.04917025938630104, 0.04735106602311134, 0.0450439490377903, 0.0447448194026947, 0.04377242177724838, 0.03928673267364502, 0.04376448690891266, 0.04051599279046059, 0.0946996882557869, 0.05417216196656227, 0.047156672924757004, 0.04654369503259659, 0.04232284426689148, 0.04762841761112213, 0.0450616255402565, 0.040318749845027924, 0.040819380432367325, 0.04770677909255028, 0.08981182426214218, 0.056038662791252136, 0.050179481506347656, 0.0505591481924057, 0.042597752064466476, 0.04423211142420769, 0.04715855047106743, 0.04241523519158363, 0.05415642261505127, 0.04507394880056381, 0.09890403598546982, 0.05588553845882416, 0.045329321175813675, 0.04740818589925766, 0.044688303023576736, 0.04035373777151108, 0.046901997178792953, 0.05075112357735634, 0.043620821088552475, 0.04485567659139633],
            title="Betazero Model Training Loss")

# plot_losses([0.5782675743103027, 0.3266085982322693, 0.18459010124206543, 0.12312400341033936, 0.09319615364074707, 0.06239180639386177, 0.06047137826681137, 0.0553707592189312, 0.06415237486362457, 0.04679011553525925, 0.3774568736553192, 0.12094663828611374, 0.0820886492729187, 0.07189799845218658, 0.0644804984331131, 0.05607682466506958, 0.059748269617557526, 0.06197406351566315, 0.047294486314058304, 0.054442938417196274, 0.1982623040676117, 0.09371814876794815, 0.08400154113769531, 0.08159155398607254, 0.05855468288064003, 0.060665227472782135, 0.05922114476561546, 0.05551839619874954, 0.06488312780857086, 0.05747784674167633, 0.11554189026355743, 0.07869622111320496, 0.0632365420460701, 0.05449412018060684, 0.06052800640463829, 0.05497833713889122, 0.04614540934562683, 0.047913458198308945, 0.05055839195847511, 0.046728380024433136, 0.12239842116832733, 0.057138897478580475, 0.04344399273395538, 0.046242501586675644, 0.04439052194356918, 0.04178005829453468, 0.04336615651845932, 0.04302532598376274, 0.04067939519882202, 0.03659560903906822, 0.11081380397081375, 0.055513471364974976, 0.04576023295521736, 0.051055338233709335, 0.04675300419330597, 0.04605131596326828, 0.04230761528015137, 0.04002254828810692, 0.04141939803957939, 0.041678134351968765, 0.15426352620124817, 0.057790905237197876, 0.04917025938630104, 0.04735106602311134, 0.0450439490377903, 0.0447448194026947, 0.04377242177724838, 0.03928673267364502, 0.04376448690891266, 0.04051599279046059, 0.0946996882557869, 0.05417216196656227, 0.047156672924757004, 0.04654369503259659, 0.04232284426689148, 0.04762841761112213, 0.0450616255402565, 0.040318749845027924, 0.040819380432367325, 0.04770677909255028, 0.08981182426214218, 0.056038662791252136, 0.050179481506347656, 0.0505591481924057, 0.042597752064466476, 0.04423211142420769, 0.04715855047106743, 0.04241523519158363, 0.05415642261505127, 0.04507394880056381, 0.09890403598546982, 0.05588553845882416, 0.045329321175813675, 0.04740818589925766, 0.044688303023576736, 0.04035373777151108, 0.046901997178792953, 0.05075112357735634, 0.043620821088552475, 0.04485567659139633],
            # title="Betazero Policy Head Training Loss")

if __name__ != "__main__":
    model = load_model("model")

    losses, p_loss, v_loss = [], [], []
    
    for h in load_and_train_dir(model):
        print("Received: {}".format(h.history))
        save_history(h, losses, p_loss, v_loss)
    plot_losses(losses, "Betazero Training Loss")
    plot_losses(v_loss, "Betazero Value Head Training Loss")
    plot_losses(p_loss, "Betazero Policy Head Training Loss")




if __name__ != "__main__":
    model = load_model("model")

    losses, p_loss, v_loss = [], [], []
    
    for h in load_and_train_dir(model):
        print("Received: {}".format(h.history))
        save_history(h, losses, p_loss, v_loss)

    if len(sys.argv) == 1:
        history = load_and_train_dir(model)
        save_history(history, losses, p_loss, v_loss)
        model.save("model", save_format="tf", signatures=model.signatures)
    
    for _ in range(10):
        import time
        import subprocess
        # p = subprocess.Popen("cargo run --release", shell=True)
        # p.wait()
        time.sleep(1)

        model = load_model("model")
        history = load_and_train(model)
        save_history(history, losses, p_loss, v_loss)
        model.save("model", save_format="tf", signatures=model.signatures)
        
        time.sleep(5)

        print(losses)
    plot_losses(losses, "Betazero Training Loss")
    plot_losses(v_loss, "Betazero Value Head Training Loss")
    plot_losses(p_loss, "Betazero Policy Head Training Loss")
